{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42834217",
   "metadata": {},
   "source": [
    "# Exercise on Regression\n",
    "\n",
    "## Exercise 2: Gradient Descent\n",
    "\n",
    "This exercise aims to serve as an introduction to Gradient Descent algorthims and nonlinear Regression\n",
    "\n",
    "We aim to find the minimum of a function: f(x,y) = sin(x)*cos(y).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b988fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd6272",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "Let's visualize the function we aim to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4024a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.linspace(-10, 10, 1000)\n",
    "y= np.linspace(-10, 10, 1000)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = np.sin(X) *np.cos(Y)\n",
    "\n",
    "# Create Plots\n",
    "fig= plt.figure(figsize=(10,5))\n",
    "\n",
    "# Subplot 1: 3D Parametric Plot\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.grid()\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=plt.get_cmap('viridis'),alpha= 0.5 )\n",
    "ax.set_title('3D Parametric Plot')\n",
    "\n",
    "ax.set_xlabel('x', labelpad=20)\n",
    "ax.set_ylabel('y', labelpad=20)\n",
    "ax.set_zlabel('f', labelpad=20)\n",
    "\n",
    "\n",
    "# Subplot 2: 2D Contour Plot\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "surf = ax.contourf(X, Y, Z, cmap=plt.get_cmap('viridis') )\n",
    "ax.set_title('2D Contour Plot')\n",
    "fig.colorbar(surf, shrink=0.5, aspect=8)\n",
    "\n",
    "# Set axes label\n",
    "ax.set_xlabel('x', labelpad=20)\n",
    "ax.set_ylabel('y', labelpad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb9264",
   "metadata": {},
   "source": [
    "#### Define the function to be minimized and its gradient:\n",
    " - function`f(indepvar)`:\n",
    "     - input : `indepvar` = array of (2,) $\\boldsymbol{x} = [x,y]$\n",
    "     - output: `z` = array of (1,)\n",
    " - function`grad_f(indepvar)`:\n",
    "     - input : `indepvar` = array of (2,) $\\boldsymbol{x} = [x,y]$\n",
    "     - output: `grad` = array of (???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00666233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the function to be minimized:\n",
    "# def f(indepvar):\n",
    "#     # ...\n",
    "#     return z\n",
    "\n",
    "# # Define the partial derivatives of the function with respect to x and y\n",
    "# def grad_f(indepvar):\n",
    "#    # ...\n",
    "#     return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a100d9",
   "metadata": {},
   "source": [
    "#### 1. Gradient descent:\n",
    "\n",
    "To perform the gradient descent, we are going to create a function that:\n",
    "- Inputs:\n",
    "    - The starting point:`start_indepvar`\n",
    "    - The learning rate : `learning_rate`\n",
    "    - The number of iterations : `num_iterations`\n",
    "\n",
    "- The function iterates until arriving convergence. Each iteration:\n",
    "   - Calculate the function value $\\boldsymbol{z}_k = f(\\boldsymbol{x}_k)$\n",
    "   - Calculate next point as $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\delta* \\nabla f(\\boldsymbol{x}_k)$\n",
    "   - Check convergence :  $|f(\\boldsymbol{x}_{k+1} - f(\\boldsymbol{x}_k)|< \\epsilon$\n",
    "   - Save the solution : `history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the gradient descent algorithm\n",
    "# def gradient_descent(start_indepvar, learning_rate, num_iterations):\n",
    "\n",
    "#     # Initialize values\n",
    "#     indepvar0 = ...\n",
    "#     z0        = ...\n",
    "    \n",
    "#     #Parameters\n",
    "#     epsilon = 0.001\n",
    "#     history = np.zeros((num_iterations, 3) )\n",
    "    \n",
    "#     # Perform the gradient descent iterations\n",
    "#     for i in range(num_iterations):\n",
    "#         # Calculate the gradients\n",
    "#         grad = ...\n",
    "        \n",
    "#         # Update values\n",
    "#         indepvar = ...\n",
    "#         z        = ...\n",
    "        \n",
    "#         # Save the history of the parameters\n",
    "#         history[i,:]=  ...\n",
    "        \n",
    "#         # Check convergence \n",
    "#         if (abs(z-z0)<epsilon ):\n",
    "#             #reshape history and break\n",
    "#             history = history[:i,:]\n",
    "#             break\n",
    "#         else:\n",
    "#             # reinitialize state 0:\n",
    "#             ...\n",
    "    \n",
    "#     print(f'Last iteration n= {i+1}, abs(z-z0)= {abs(z-z0)}')\n",
    "#     return indepvar, z, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b4cad",
   "metadata": {},
   "source": [
    "Perform gradient descent and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e75cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform gradient descent and plot the results\n",
    "# start_xy = [3, 3]\n",
    "# learning_rate = 0.1\n",
    "# num_iterations = 100\n",
    "# indepvar_opt, f_opt, history = gradient_descent(start_xy, learning_rate, num_iterations)\n",
    "\n",
    "\n",
    "\n",
    "# Create Plots\n",
    "fig= plt.figure(figsize=(10,5))\n",
    "\n",
    "# Subplot 1: 3D Parametric Plot\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.grid()\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=plt.get_cmap('viridis'),alpha= 0.5 )\n",
    "ax.set_title('3D Parametric Plot')\n",
    "ax.scatter(*zip(*history), c='k', marker='o', s=5)\n",
    "ax.scatter(*history[0], c='r', marker='o')\n",
    "ax.scatter(*history[len(history)-1], c='b', marker='o')\n",
    "\n",
    "ax.set_xlabel('x', labelpad=20)\n",
    "ax.set_ylabel('y', labelpad=20)\n",
    "ax.set_zlabel('f', labelpad=20)\n",
    "\n",
    "\n",
    "# Subplot 2: 2D Contour Plot\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "surf = ax.contourf(X, Y, Z, cmap=plt.get_cmap('viridis') )\n",
    "ax.set_title('2D Contour Plot')\n",
    "fig.colorbar(surf, shrink=0.5, aspect=8)\n",
    "\n",
    "nit= history.shape[0]\n",
    "\n",
    "for it in range(nit-1):\n",
    "    plt.scatter(history[it,0], history[it,1], c='k', alpha=0.2)\n",
    "\n",
    "plt.scatter(history[0,0], history[0,1] , c='r') # init\n",
    "plt.scatter(history[nit-1,0], history[nit-1,1] , c='b') # end\n",
    "\n",
    "\n",
    "# Set axes label\n",
    "ax.set_xlabel('x', labelpad=20)\n",
    "ax.set_ylabel('y', labelpad=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494da1fd",
   "metadata": {},
   "source": [
    "#### Gradient descent  estimated:\n",
    "\n",
    "Not always we know analitically the function of the gradient of the function.\n",
    "\n",
    "We will create a function called `gradient_descent_estimated` that estimates the gradient.\n",
    " - To do:\n",
    "     - Create the function similarly as before\n",
    "     - Calculate the difference between the estimated gradient and the exact one at the last iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda05b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the gradient descent algorithm with estimation of the gradient\n",
    "# def gradient_descent_estimated(start_indepvar, learning_rate, num_iterations):\n",
    "\n",
    "#     # Initialize values\n",
    "#     indepvar0 = ...\n",
    "#     z0        = ...\n",
    "    \n",
    "#     #Parameters\n",
    "#     epsilon = 0.001\n",
    "#     history = np.zeros((num_iterations, 3) )\n",
    "    \n",
    "#     # Perform the gradient descent iterations\n",
    "#     for i in range(num_iterations):\n",
    "#         # Calculate the gradients\n",
    "#         ...\n",
    "#         ...\n",
    "#         grad = ...\n",
    "        \n",
    "#         # Update values\n",
    "#         indepvar = ...\n",
    "#         z        = ...\n",
    "        \n",
    "#         # Save the history of the parameters\n",
    "#         history[i,:]=  ...\n",
    "        \n",
    "#         # Check convergence \n",
    "#         if (abs(z-z0)<epsilon ):\n",
    "#             #reshape history and break\n",
    "#             history = history[:i,:]\n",
    "#             break\n",
    "#         else:\n",
    "#             # reinitialize state 0:\n",
    "#             ...\n",
    "            \n",
    "#     print(f'Last iteration n= {i+1}, abs(z-z0)= {abs(z-z0)}')\n",
    "#     print(f'diff in grad = {grad - grad_f(indepvar)}')\n",
    "    \n",
    "#     return indepvar, z, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15795824",
   "metadata": {},
   "source": [
    "Perform gradient descent estimating the gradient and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent and plot the results\n",
    "start_xy = [3, 3]\n",
    "learning_rate = 0.1\n",
    "num_iterations = 1000\n",
    "indepvar_opt, f_opt, history = gradient_descent_estimated(start_xy, learning_rate, num_iterations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create Plots\n",
    "fig= plt.figure(figsize=(10,5))\n",
    "\n",
    "# Subplot 1: 3D Parametric Plot\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.grid()\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=plt.get_cmap('viridis'),alpha= 0.5 )\n",
    "ax.set_title('3D Parametric Plot')\n",
    "ax.scatter(*zip(*history), c='k', marker='o', s=5)\n",
    "ax.scatter(*history[0], c='r', marker='o')\n",
    "ax.scatter(*history[len(history)-1], c='b', marker='o')\n",
    "\n",
    "ax.set_xlabel('x', labelpad=20)\n",
    "ax.set_ylabel('y', labelpad=20)\n",
    "ax.set_zlabel('f', labelpad=20)\n",
    "\n",
    "\n",
    "# Subplot 2: 2D Contour Plot\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "surf = ax.contourf(X, Y, Z, cmap=plt.get_cmap('viridis') )\n",
    "ax.set_title('2D Contour Plot')\n",
    "fig.colorbar(surf, shrink=0.5, aspect=8)\n",
    "\n",
    "nit= history.shape[0]\n",
    "\n",
    "for it in range(nit-1):\n",
    "    plt.scatter(history[it,0], history[it,1], c='k', alpha=0.2)\n",
    "\n",
    "plt.scatter(history[0,0], history[0,1] , c='r') # init\n",
    "plt.scatter(history[nit-1,0], history[nit-1,1] , c='b') # end\n",
    "\n",
    "\n",
    "# Set axes label\n",
    "ax.set_xlabel('x', labelpad=20)\n",
    "ax.set_ylabel('y', labelpad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812289df",
   "metadata": {},
   "source": [
    "#### Gradient descent  scipy.optimize:\n",
    "\n",
    "We will use scipy.optimize library to obtain the minimum of our function\n",
    "\n",
    "- To do: implement the function `scipy.optimize.fmin` to obtain the value of 'indepvar' that minimizes the function f(indepvar)\n",
    "\n",
    "Check the documentation:\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "start_xy = [3, 3]\n",
    "# [... ] = scipy.optimize.fmin( ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Plots\n",
    "fig= plt.figure(figsize=(10,5))\n",
    "\n",
    "# Subplot 1: 3D Parametric Plot\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.grid()\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=plt.get_cmap('viridis'),alpha= 0.5 )\n",
    "ax.set_title('3D Parametric Plot')\n",
    "\n",
    "ax.set_xlabel('x', labelpad=20)\n",
    "ax.set_ylabel('y', labelpad=20)\n",
    "ax.set_zlabel('f', labelpad=20)\n",
    "\n",
    "\n",
    "# Subplot 2: 2D Contour Plot\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "surf = ax.contourf(X, Y, Z, cmap=plt.get_cmap('viridis') )\n",
    "ax.set_title('2D Contour Plot')\n",
    "fig.colorbar(surf, shrink=0.5, aspect=8)\n",
    "\n",
    "nit= history.shape[0]\n",
    "\n",
    "it=0; plt.scatter(indepvar_all[it][0], indepvar_all[it][1] , c='r') #init\n",
    "\n",
    "for it in range(nit):\n",
    "    plt.scatter(indepvar_all[it][0], indepvar_all[it][1],  c='k', alpha=0.5)\n",
    "\n",
    "plt.scatter(indepvar_all[it][0], indepvar_all[it][1], c='b') #end\n",
    "\n",
    "ax.set_xlabel('x', labelpad=20)\n",
    "ax.set_ylabel('y', labelpad=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
